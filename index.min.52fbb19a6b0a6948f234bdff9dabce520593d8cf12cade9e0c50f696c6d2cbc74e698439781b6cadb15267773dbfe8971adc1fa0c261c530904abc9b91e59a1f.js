var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(c){const d=suggestions.classList.contains('d-none');if(d)return;const a=[...suggestions.querySelectorAll('a')];if(a.length===0)return;const b=a.indexOf(document.activeElement);if(c.key==="ArrowUp"){c.preventDefault();const d=b>0?b-1:0;a[d].focus()}else if(c.key==="ArrowDown"){c.preventDefault();const d=b+1<a.length?b+1:b;a[d].focus()}}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/prologue/welcome/",title:"Welcome!",description:"Introduction to MLOps",content:""}).add({id:1,href:"/docs/introduction/",title:"Introduction",description:"Introduction to MLOps",content:""}).add({id:2,href:"/docs/introduction/intro/",title:"What is MLOps?",description:"Introduction to MLOps",content:"서론 # 최근 MLOps와 관련된 주제의 세미나와 글들이 많아지고 모두 MLOps의 필요성에 대해 말하고 있습니다.\n그런데 MLOps란 무엇이며 이를 위해서 우리는 무엇을 공부해야 할까요?\n저희 모두의 MLOps는 MLOps에 대해서 공부하려고 하지만 어떻게 시작해야 하는지 모르는 분들을 위한 지침서를 작성하고자 이 프로젝트를 시작하였습니다.\nMLOps는 Machine Learning Operations의 약어입니다. Operations는 도메인에 따라서 또는 상황에 따라서 필요로 하는 것이 달라진다는 뜻을 내포하고 있습니다.\n특히 집필을 하는 2022년 기준으로 아직 표준이라고 불릴 수 있는 MLOps 툴이 존재하지 않습니다. 그렇기 때문에 저희가 제시하는 방법을 실제 업무에 바로 적용하기에는 힘들 수도 있습니다.\n그럼에도 이 글을 통해서 많은 분들이 MLOps란 무엇이며 각자의 환경에 맞게 어떤 것이 필요한 지를 알 수 있는 첫 디딤돌이 되었으면 합니다.\nMachine Learning Project # 2012년 Alexnet 이후 CV, NLP, Tabular Data등 데이터가 존재하는 곳에서는 모두 머신러닝과 딥러닝을 도입하고자 하였습니다.\n딥러닝과 머신러닝은 AI라는 단어로 묶이며 불렸고 많은 매체에서 AI의 필요성을 외쳤습니다. 그래서 무수히 만흔 기업에서 머신러닝과 딥러닝을 이용한 수 많은 프로젝트를 진행하였습니다. 그리고 어떻게 되었을까요?\n엘리먼트 AI의 음병찬 동북아 지역 총괄책임자는 \u0026ldquo;10개 기업에 AI 프로젝트를 시작한다면 그 중 9개는 컨셉검증(POC)만 하다 끝난다\u0026rdquo;고 말했습니다.\n이처럼 많은 프로젝트에서 머신러닝과 딥러닝은 이 문제를 풀 수 있을 것 같다는 가능성만을 보여주고 사라졌습니다. 그리고 이 시기 즈음에 AI에게 다시 겨울이 다가오고 있다는 전망들도 나오기 시작했습니다.\n왜 대부분의 프로젝트가 컨셉검증(POC) 단계에서 끝났을까요?\n머신러닝과 딥러닝 코드만으로는 실제 서비스를 운영할 수 없기 때문입니다. 실제 서비스 단계에서 머신러닝과 딥러닝의 코드가 차지하는 부분은 생각보다 작으며 단순히 모델만이 아니 다른 많은 부분들을 고려해야 합니다.\n구글은 이런 문제를 2015년 Hidden Technical Debt in Machine Learning Systems에서 지적한 바 있습니다.\n하지만 이 논문이 나올 때는 아직 많은 머신러닝 엔지니어들이 딥러닝과 머신러닝의 가능성을 입증하기 바쁜 시기였기 때문에, 논문이 지적하는 바에 많은 주의를 기울이지는 않았습니다.\n몇 년이 지난 후 머신러닝과 딥러닝은 가능성을 입증을 해냈고 사람들은 이제 실제 서비스에 적용하고자 했습니다.\n그리고 많은 사람들이 실제 서비스는 쉽지 않다는 것을 깨달았습니다.\n Before MLOps # 사람들은 우선 머신러닝과 딥러닝을 일반적인 소프트웨어와 동일한 시선으로 바라보고 기존과 같은 방식으로 운영하고자 했습니다.\n일반적인 소프트웨어에서는 어떤 문제가 발생할 경우 해당 부분을 담당한 소프트웨어 엔지니어가 문제의 원인을 진단하고 이를 해결한 후 다시 배포를 하였습니다.\n이 때 머신러닝 모델을 배포하기 위해서는 머신러닝 엔지니어가 직접 모델을 학습한 후 모델 파일들을 배포를 담당하는 소프트웨어 엔지니어에게 전달하였습니다. 두 직군간의 소통의 매개체는 \u0026lsquo;학습된 모델\u0026rsquo;이였습니다.\n하지만 이 소통의 매개체로 인해 머신러닝 엔지니어와 소프트웨어 엔지니어들의 갈등이 시작되었습니다.\n머신러닝 엔지니어와 소프트웨어 엔지니어들은 모델(Network 구조와 Weights 가 담긴 파일) 을 매개체로 서로 소통했습니다.\n머신러닝 엔지니어들은 DB에서 직접 쿼리를 이용해 데이터를 다운로드 받고 모델을 학습 후, 학습된 모델을 소프트웨어 엔지니어에게 전달하였습니다.\n그러면 소프트웨어 엔지니어는 전달받은 모델을 로드한 뒤, 정해진 추론(inference) 함수를 제공하는 API Server 를 만들어 배포하였습니다.\n이 과정에서 소프트웨어 엔지니어는 머신러닝 엔지니어에게 정해진 형식에 맞춰서 구현할 것을 요청합니다. 예를 들면 OS, 파이썬 버전, 사용한 패키지, 클래스 구조 등을 포함합니다.\n소프트웨어 엔지니어와 머신러닝 엔지니어는 서로 어떤 환경에서 작업을 하는지 알지 못하기 때문에 소통의 과정에서 미리 약속한 형식에서 하나라도 어긋난다면 배포와 성능 재현의 문제가 발생합니다.\n예를 들어서 개발 환경에서는 동작했던 코드가 실행 환경에서는 동작하지 않는다던지, 개발 환경에서과 동일한 성능이 재현되지 않는 문제가 발생하게 됩니다.\n또한 Data Shift와 같이 학습에 사용된 데이터와 다른 데이터가 들어왔을 때 모델의 성능이 감소하는 문제가 생겼습니다.\n일반적인 소프트웨어에서 문제를 해결하는 방법과 비슷하게 모델을 담당한 머신러닝 엔지니어들이 모델 성능 하락의 원인을 찾고 이를 개선한 새로운 모델을 배포했습니다.\n일반적인 경우 최신 데이터를 추가적으로 반영하여 모델을 재학습하는 것만으로도 모델의 성능의 하락을 개선할 수 있었기 때문에, 머신러닝 엔지니어들은 주기적으로 모델을 재학습해서 배포하였습니다.\nAfter MLOps # MLOps에서는 파이프라인을 이용해 서로 소통합니다. 여기서 파이프라인이란 단순히 학습된 모델만이 아닌 모델에 종속된 모든 작업을 포함한 개념입니다.\nContinuous Integration \u0026amp; Deployment # 머신러닝 엔지니어는 데이터를 다운로드 받고, 전처리를 수행하며, 모델을 생성하기까지의 모든 과정을 파이프라인의 형태로 작성하여 소프트웨어 엔지니어에게 전달하고, 소프트웨어 엔지니어는 전달받은 파이프라인에서 생성된 모델을 배포합니다.\n머신러닝 엔지니어와 소프트웨어 엔지니어는 같은 파이프라인을 사용하기 때문에 언제 어디서나 동일한 성능의 모델을 빌드하고 배포하는 것을 보장할 수 있게 됩니다.\nContinuous Training # 머신러닝 모델은 녹이 습니다. 이 말인 즉슨 한 번 학습되어 뛰어난 성능을 보였던 모델이라고 하더라도, 시간이 지남에 따라 성능이 저하되는 현상이 있다는 말입니다. 모델의 성능이 저하되는 근본적인 이유는 데이터의 분포가 변화하기 때문입니다.\n데이터의 분포는 왜 변할까요? 데이터의 분포가 변화하는 이유로는 데이터의 모분포를 모르기 때문일수도 있고 현실에서 얻어지는 데이터의 특성은 변화할 수 있기 때문입니다.\n이 때문에 최신 데이터를 이용해 모델을 재학습하여 모델의 성능을 다시 끌어올리는 작업이 필요하게 되었고, 머신러닝 엔지니어가 항상 이 작업을 반복하기보다는 파이프라인을 통해서 자동화를 할 수 있습니다.\n"}).add({id:3,href:"/docs/introduction/component/",title:"MLOps의 구성 요소",description:"Describe MLOps Components",content:"Practitioners guide to MLOps # 2021년 5월에 발표된 구글의 white paper : Practitioners guide to MLOps: A framework for continuous delivery and authmation of machine learning에서는 MLOps의 핵심 기능들로 다음과 같은 것들을 언급하였습니다.\n 각 기능들이 어떤 역할을 하는지 살펴보겠습니다.\n1. Experimentation # 실험(Experimentation)은 머신러닝 엔지니어들이 데이터를 분석하고, 프로토타입 모델을 만들며 학습 기능을 구현할 수 있도록 하는 다음과 같은 기능을 제공합니다.\n 깃(Git)과 같은 버전 컨트롤 툴과 통합된 노트북(Jupyter Notebook) 환경 제공 사용한 데이터, 하이퍼 파라미터, 평가 지표를 포함한 실험 추적 기능 제공 데이터와 모델에 대한 분석 및 시각화 기능 제공  2. Data Processing # 데이터 처리(Data Processing)은 머신러닝 모델 개발 단계, 지속적인 학습(Continuous Training) 단계, 그리고 API 배포(API Deployment) 단계에서 많은 양의 데이터를 사용할 수 있게 해 주는 다음과 같은 기능을 제공합니다.\n 다양한 데이터 소스와 서비스에 호환되는 데이터 커넥터(connector) 기능 제공 다양한 형태의 데이터와 호환되는 데이터 인코터(encoder) \u0026amp; 디코더(decoder) 기능 제공 다양한 형태의 데이터에 대한 데이터 변환과 피쳐 엔지니어링(feature engineering) 기능 제공 학습과 서빙을 위한 확장가능한 배치, 스트림 데이터 처리 기능 제공  3. Model training # 모델 학습(Model training)은 모델 학습을 위한 알고리즘을 효율적으로 실행시켜주는 다음과 같은 기능을 제공합니다.\n ML 프레임워크의 실행을 위한 환경 제공 다수의 GPU / 분산 학습 사용을 위한 분산 학습 환경 제공 하이퍼파라미터 튜닝과 최적화 기능 제공  4. Model evaluation # 모델 평가(Model evaluation)은 실험 환경과 상용 환경에서 동작하는 모델의 성능을 관찰할 수 있는 다음과 같은 기능을 제공합니다.\n 평가 데이터에 대한 모델 성능 평가 기능 서로 다른 지속 학습 실행결과에 대한 예측 성능 추적 서로 다른 모델의 성능 비교와 시각화 해석 가능한 AI 기술을 이용한 모델 출력 해석 기능 제공  5. Model serving # 모델 서빙(Model serving)은 상용 환경에 모델을 배포하고 서빙하기 위한 다음과 같은 기능들을 제공합니다.\n 저지연 추론과 고가용성 추론 기능 제공 다양한 ML 모델 서빙 프레임워크 지원(Tensorflow Serving, TorchServe, Nvidia Triton, Scikit-learn, XGGoost .. etc) 복잡한 형태의 추론 루틴 기능 제공, 예를 들어 전처리(preprocess) 또는 후처리(postprocess) 기능과 최종 결과를 위해 다수의 모델이 사용되는 경우를 말한다. 순간적으로 치솟는 추론 요청을 처리하기 위한 오토 스케일링(autoscaling) 기능 제공 추론 요청과 추론 결과에 대한 로깅 기능 제공  6. Online experimentation # 온라인 실험(Online experimentation)은 새로운 모델이 생성되었을 때, 이 모델을 배포하면 어느 정도의 성능을 보일 것인지 검증하는 기능을 제공한다. 이 기능은 새 모델을 배포하는 것 까지 연동하기 위해 모델 저장소(Model Registry)와 연동되어야 한다.\n 카나리(canary) \u0026amp; 섀도우(shadow) 배포 기능 제공 A/B 테스트 기능 제공 멀티 암드 밴딧(Multi-armed bandit) 테스트 기능 제공  7. Model Monitoring # 모델 모니터링(Model Monitoring)은 상용 환경에 배포되어 있는 모델이 정상적으로 동작하고 있는지를 모니터링 하는 기능을 제공한다. 이 기능은 모델의 성능이 떨어져 업데이트가 필요한지에 대한 정보를 제공해준다.\n8. ML Pipeline # 머신러닝 파이프라인(ML Pipeline)은 상용 환경에서 복잡한 ML 학습과 추론 작업을 구성하고 제어하고 자동화 하기 위한 다음과 같은 기능을 제공한다.\n 다양한 이벤트를 소스를 통한 파이프라인 실행 기능 파이프라인 파라미터와 생성되는 산출물 관리를 위한 머신러닝 메타데이터 추적과 연동 기능 일반적인 머신러닝 작업을 위한 내장 컴포넌트 지원과 사용자가 직접 구현한 컴포넌트에 대한 지원 기능 서로 다른 실행 환경 제공 기능  9. Model Registry # 모델 저장소(Model Registry)는 머신러닝 모델의 생명주기(Lifecycle)을 중앙 저장소에서 관리할 수 있게 해 주는 기능을 제공합니다.\n 학습된 모델 그리고 배포된 모델에 대한 등록, 추적, 버저닝 기능 제공 배포를 위해 필요한 데이터와 런타임 패키지들에 대한 정보 저장 기능  10. Dataset and Feature Repository #  데이터에 대한 공유, 검색, 재사용 그리고 버전관리 기능 이벤트 스트리밍 및 온라인 추론 작업에 대한 실시간 처리 및 저지연 서빙 기능 사진, 텍스트, 테이블 형태의 데이터와 같은 다양한 형태의 데이터 지원 기능  11. ML Metadata and Artifact Tracking # MLOps의 각 단계에서 다양한 형태의 산출물들이 생성된다. ML 메타데이터는 이런 산출물들에 대한 정보를 의미한다. ML 메타데이터와 산출물 관리는 산출물의 위치, 타입, 속성, 그리고 관련된 실험(experiment)에 대한 정보를 관리하기 위해 다음과 같은 기능들을 제공해준다.\n ML 산출물에 대한 히스토리 관리 기능 실험과 파이프라인 파라미터 설정에 대한 추적, 공유 기능 ML 산출물에 대한 저장, 접근, 시각화, 다운로드 기능 제공 기타 다른 MLOps 기능과의 통합 기능 제공  "}).add({id:4,href:"/docs/introduction/why_kubernetes/",title:"Why Kuberntes?",description:"Reason for using k8s in MLOps",content:"MLOps \u0026amp; Kubernetes # 그렇다면 MLOps를 이야기할 때, 쿠버네티스(Kubernetes)라는 단어가 항상 함께 들리는 이유가 무엇일까요?\n성공적인 MLOps 시스템을 구축하기 위해서는 MLOps의 구성요소 에서 설명한 것처럼 다양한 구성 요소들이 필요하지만, 각각의 구성 요소들이 유기적으로 운영되기 위해서는 인프라 레벨에서 수많은 이슈들을 해결해야 합니다.\n간단하게는 수많은 머신러닝 모델의 학습 요청을 순차적으로 실행 하는 것, 다른 작업 공간에서도 동일한 실행 환경을 보장해야 하는 것, 배포된 서비스에 장애가 생겼을 때 빠르게 대응해야 하는 것 등의 이슈 등을 생각해볼 수 있습니다.\n여기서 컨테이너(Container)와 컨테이너 오케스트레이션 시스템(Container Orchestration System)의 필요성이 등장합니다.\n쿠버네티스와 같은 컨테이너 오케스트레이션 시스템을 도입하면 실행 환경의 격리와 관리를 효율적으로 수행할 수 있습니다. 컨테이너 오케스트레이션 시스템을 도입한다면, 머신러닝 모델을 개발하고 배포하는 과정에서 다수의 개발자가 소수의 서버를 공유하면서 \u0026lsquo;1번 서버 사용 중이신가요?\u0026rsquo;, \u0026lsquo;GPU 사용 중이던 제 프로세스 누가 죽였나요?\u0026rsquo;, \u0026lsquo;누가 서버에 x 패키지 업데이트 했나요?' 와 같은 상황을 방지할 수 있습니다.\nContainer # 그렇다면 컨테이너란 무엇일까요? 마이크로소프트에서는 컨테이너를 다음과 같이 정의하고 있습니다.\n 컨테이너란 : 애플리케이션의 표준화된 이식 가능한 패키징\n 그런데 왜 머신러닝에서 컨테이너가 필요할까요? 머신러닝 모델들은 운영체제나 Python 실행 환경, 패키지 버전 등에 따라 다르게 동작할 수 있습니다.\n이를 방지하기 위해서 머신러닝에 사용된 소스코드와 함께 종속적인 실행 환경 전체를 하나로 묶어서(패키징해서) 공유하고 실행하는 데 활용할 수 있는 기술이 컨테이너라이제이션(Containerization) 기술입니다. 이렇게 패키징된 형태를 컨테이너 이미지라고 부르며, 컨테이너 이미지를 공유함으로써 사용자들은 어떤 시스템에서든 동일한 실행 결과를 보장할 수 있게 됩니다.\n즉, 단순히 Jupyter Notebook 파일이나, 모델의 소스코드와 requirements.txt 파일을 공유하는 것이 아닌, 모든 실행 환경이 담긴 컨테이너 이미지를 공유한다면 \u0026ldquo;제 노트북에서는 잘 되는데요?\u0026quot; 와 같은 상황을 피할 수 있습니다.\n컨테이너를 처음 접하시는 분들이 흔히 하시는 오해 중 하나는 \u0026ldquo;컨테이너 == 도커\u0026ldquo;라고 받아들이는 것입니다.\n도커는 컨테이너와 동일한 의미를 지니는 개념이 아니라, 컨테이너를 띄우거나, 컨테이너 이미지를 만들고 공유하는 것과 같이 컨테이너를 보다 쉽고 유연하게 사용할 수 있는 기능을 제공해주는 도구입니다. 정리하자면 컨테이너는 가상화 기술이고, 도커는 가상화 기술의 구현체라고 말할 수 있습니다.\n다만, 도커는 여러 컨테이너 가상화 도구 중에서 쉬운 사용성과 높은 효율성을 바탕으로 가장 빠르게 성장하여 대세가 되었기에 컨테이너하면 도커라는 이미지가 자동으로 떠오르게 되었습니다. 이렇게 컨테이너와 도커 생태계가 대세가 되기까지는 다양한 이유가 있지만, 기술적으로 자세한 이야기는 모두의 MLOps의 범위를 넘어서기 때문에 다루지는 않겠습니다.\n컨테이너 혹은 도커를 처음 들어보시는 분들에게는 모두의 MLOps의 내용이 다소 어렵게 느껴질 수 있기 때문에, 생활코딩, subicura 님의 개인 블로그 글 등의 자료를 먼저 살펴보는 것을 권장합니다.\nContainer Orchestration System # 그렇다면 컨테이너 오케스트레이션 시스템은 무엇일까요? 오케스트레이션이라는 단어에서 추측해 볼 수 있듯이, 수많은 컨테이너들이 있을 때 컨테이너들이 서로 조화롭게 구동될 수 있도록 지휘하는 시스템에 비유할 수 있습니다.\n컨테이너를 도입이 되면 서비스는 컨테이너의 형태로 사용자들에게 제공됩니다. 이 때 관리해야 할 컨테이너의 수가 적다면 운영 담당자 한 명이서도 충분히 모든 상황에 대응할 수 있습니다.\n하지만, 수 백 개 이상의 컨테이너가 수 십 대 이상의 서버에서 구동되고 있고 장애를 일으키지 않고 항상 정상 동작해야 한다면, 모든 서비스의 정상 동작 여부를 담당자 한 명이 파악하고 이슈에 대응하는 것은 불가능에 가깝습니다.\n예를 들면, 모든 서비스가 정상적으로 동작하고 있는지를 계속해서 모니터링(Monitoring)해야 합니다.\n만약, 특정 서비스가 장애를 일으켰다면 여러 컨테이너들의 로그를 확인해가며 문제를 파악해야 합니다.\n또한 특정 서버나 특정 컨테이너에 작업이 몰리지 않도록 스케줄링(Scheduling)하고 로드 밸런싱(Load Balancing)하며, 스케일링(Scaling)하는 등의 수많은 작업을 담당해야 합니다. 이렇게 수많은 컨테이너들의 상태를 지속적으로 관리하고 운영하는 과정을 조금이나마 쉽게, 자동으로 할 수 있는 기능을 제공해주는 소프트웨어가 바로 컨테이너 오케스트레이션 시스템입니다.\n머신러닝에서는 어떻게 쓰일 수 있을까요?\n예를 들어서 GPU를 필요로 하는 딥러닝 학습 코드가 패키징된 컨테이너는 사용 가능한 GPU가 있는 서버에서 수행하고, 많은 메모리를 필요로 하는 데이터 전처리 코드가 패키징된 컨테이너는 메모리의 여유가 많은 서버에서 수행하고, 학습 중에 서버에 문제가 생기면 자동으로 동일한 컨테이너를 다른 서버로 이동시키고 다시 학습을 진행하는 등의 작업을 사람이 일일히 수행하지 않고, 자동으로 관리하는 시스템을 개발한 뒤 맡기는 것입니다.\n집필을 하는 2022년을 기준으로 쿠버네티스는 컨테이너 오케스트레이션 시스템의 사실상의 표준(De facto standard)입니다.\nCNCF에서 2018년 발표한 Survey 에 따르면 다음 그림과 같이 이미 두각을 나타내고 있었으며, 2019년 발표한 Survey에 따르면 그 중 78% 가 상용 수준(Production Level)에서 사용하고 있다는 것을 알 수 있습니다.\n쿠버네티스 생태계가 이처럼 커지게 된 이유에는 여러 가지 이유가 있습니다. 하지만 도커와 마찬가지로 쿠버네티스 역시 머신러닝 기반의 서비스에서만 사용하는 기술이 아니기에, 자세히 다루기에는 상당히 많은 양의 기술적인 내용을 다루어야 하므로 이번 모두의 MLOps에서는 자세한 내용은 생략할 예정입니다.\n다만, 모두의 MLOps에서 앞으로 다룰 내용은 도커와 쿠버네티스에 대한 내용을 어느 정도 알고 계신 분들을 대상으로 작성하였습니다. 따라서 쿠버네티스에 대해 익숙하지 않으신 분들은 다음 쿠버네티스 공식 문서, subicura 님의 개인 블로그 글 등의 쉽고 자세한 자료들을 먼저 참고해주시는 것을 권장합니다.\n"}).add({id:5,href:"/docs/setup/",title:"Setup",description:"Setup kubernetes.",content:""}).add({id:6,href:"/docs/setup/example/",title:"Setup example",description:"Introduction to MLOps",content:"예시 페이지 입니다. 헤딩은 2번 부터 시작해주세요. # "}).add({id:7,href:"/docs/kubeflow/",title:"Kubeflow",description:"How to use Kubeflow.",content:""}).add({id:8,href:"/docs/kubeflow/example/",title:"Kubeflow example",description:"Introduction to MLOps",content:"예시 페이지 입니다. 헤딩은 2번 부터 시작해주세요. # "}).add({id:9,href:"/docs/api-deployment/",title:"API Deployment",description:"API deployment with seldon-core",content:""}).add({id:10,href:"/docs/api-deployment/example/",title:"API Deployment example",description:"Introduction to MLOps",content:"예시 페이지 입니다. 헤딩은 2번 부터 시작해주세요. # "}).add({id:11,href:"/docs/help/",title:"Help",description:"Help Doks.",content:""}).add({id:12,href:"/docs/help/how-to-contribute/",title:"How to Contribute",description:"How to Start #  필요한 node module을 설치합니다.  npm install 글 수정 및 추가를 후 ci 를 실행합니다.  npm ci node 서버를 실행 후 수정한 글이 정상적으로 나오는지 확인합니다.  npm run start How to Contribute # 1. 새로운 포스트를 작성하는 경우 # 새로운 포스트는 각 챕터와 포스트의 위치에 맞는 weight를 설정합니다.\n Introduction: 1xx Setup: 2xx Kubeflow: 3xx API Deployment: 4xx Help: 10xx  2. 기존의 포스트를 수정하는 경우 # 기존의 포스트를 수정할 경우 Contributor에 본인의 이름을 입력합니다.",content:"How to Start #  필요한 node module을 설치합니다.  npm install 글 수정 및 추가를 후 ci 를 실행합니다.  npm ci node 서버를 실행 후 수정한 글이 정상적으로 나오는지 확인합니다.  npm run start How to Contribute # 1. 새로운 포스트를 작성하는 경우 # 새로운 포스트는 각 챕터와 포스트의 위치에 맞는 weight를 설정합니다.\n Introduction: 1xx Setup: 2xx Kubeflow: 3xx API Deployment: 4xx Help: 10xx  2. 기존의 포스트를 수정하는 경우 # 기존의 포스트를 수정할 경우 Contributor에 본인의 이름을 입력합니다.\ncontributors: [\u0026#34;John Doe\u0026#34;, \u0026#34;Adam Smith\u0026#34;] 3. 프로젝트에 처음 기여하는 경우 # 만약 프로젝트에 처음 기여 할 경우 content/en/contributors에 본인의 이름의 마크다운 파일을 작성합니다. 마크다운 파일은 john-doe을 파일명으로 하며 다음의 내용을 작성합니다. 파일 명은 lowercase를 title은 upper camelcase를 이용해 작성합니다.\n--- title: \u0026#34;Jonh Doe\u0026#34; draft: false --- Before Commit # 프로젝트에서는 각 글들의 일관성을 위해서 여러 lint를 적용하고 있습니다. 다음 명령어를 실행해 test를 진행합니다.\npre-commit을 통해 대부분의 test를 통과할 수 있습니다.\npip install pre-commit pre-commit run -a pre-commit 후 test를 진행합니다.\nnpm test "}).add({id:13,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener('input',b,!0);function b(){var b,e;const d=5;b=this.value,e=a.search(b,{limit:d,enrich:!0});const c=new Map;for(const a of e.flatMap(a=>a.result)){if(c.has(a.doc.href))continue;c.set(a.doc.href,a.doc)}if(suggestions.innerHTML="",suggestions.classList.remove('d-none'),c.size===0&&b){const a=document.createElement('div');a.innerHTML=`No results for "<strong>${b}</strong>"`,a.classList.add("suggestion__no-results"),suggestions.appendChild(a);return}for(const[h,g]of c){const b=document.createElement('div');suggestions.appendChild(b);const a=document.createElement('a');a.href=h,b.appendChild(a);const e=document.createElement('span');e.textContent=g.title,e.classList.add("suggestion__title"),a.appendChild(e);const f=document.createElement('span');if(f.textContent=g.description,f.classList.add("suggestion__description"),a.appendChild(f),suggestions.appendChild(b),suggestions.childElementCount==d)break}}})()