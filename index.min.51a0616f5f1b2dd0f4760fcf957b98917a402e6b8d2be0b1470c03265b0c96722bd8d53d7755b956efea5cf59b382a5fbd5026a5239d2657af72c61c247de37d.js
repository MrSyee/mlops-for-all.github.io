var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(c){const d=suggestions.classList.contains('d-none');if(d)return;const a=[...suggestions.querySelectorAll('a')];if(a.length===0)return;const b=a.indexOf(document.activeElement);if(c.key==="ArrowUp"){c.preventDefault();const d=b>0?b-1:0;a[d].focus()}else if(c.key==="ArrowDown"){c.preventDefault();const d=b+1<a.length?b+1:b;a[d].focus()}}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/prologue/welcome/",title:"Welcome!",description:"Introduction to MLOps",content:""}).add({id:1,href:"/docs/introduction/",title:"Introduction",description:"Introduction to MLOps",content:""}).add({id:2,href:"/docs/introduction/intro/",title:"What is MLOps?",description:"Introduction to MLOps",content:"서론 # 최근 MLOps와 관련된 주제의 세미나와 글들이 많아지고 모두 MLOps의 필요성에 대해 말하고 있습니다.\n그런데 MLOps란 무엇이며 이를 위해서 우리는 무엇을 공부해야 할까요?\n저희 모두의 MLOps는 MLOps에 대해서 공부하려고 하지만 어떻게 시작해야 하는지 모르는 분들을 위한 지침서를 작성하고자 이 프로젝트를 시작하였습니다.\nMLOps는 Machine Learning Operations의 약어입니다. Operations는 도메인에 따라서 또는 상황에 따라서 필요로 하는 것이 달라진다는 뜻을 내포하고 있습니다.\n특히 집필을 하는 2022년 기준으로 아직 표준이라고 불릴 수 있는 MLOps 툴이 존재하지 않습니다. 그렇기 때문에 저희가 제시하는 방법을 실제 업무에 바로 적용하기에는 힘들 수도 있습니다.\n그럼에도 이 글을 통해서 많은 분들이 MLOps란 무엇이며 각자의 환경에 맞게 어떤 것이 필요한 지를 알 수 있는 첫 디딤돌이 되었으면 합니다.\nMachine Learning Project # 2012년 Alexnet 이후 CV, NLP, Tabular Data등 데이터가 존재하는 곳에서는 모두 머신러닝과 딥러닝을 도입하고자 하였습니다.\n딥러닝과 머신러닝은 AI라는 단어로 묶이며 불렸고 많은 매체에서 AI의 필요성을 외쳤습니다. 그래서 무수히 만흔 기업에서 머신러닝과 딥러닝을 이용한 수 많은 프로젝트를 진행하였습니다. 그리고 어떻게 되었을까요?\n엘리먼트 AI의 음병찬 동북아 지역 총괄책임자는 \u0026ldquo;10개 기업에 AI 프로젝트를 시작한다면 그 중 9개는 컨셉검증(POC)만 하다 끝난다\u0026rdquo;고 말했습니다.\n이처럼 많은 프로젝트에서 머신러닝과 딥러닝은 이 문제를 풀 수 있을 것 같다는 가능성만을 보여주고 사라졌습니다. 그리고 이 시기 즈음에 AI에게 다시 겨울이 다가오고 있다는 전망들도 나오기 시작했습니다.\n왜 대부분의 프로젝트가 컨셉검증(POC) 단계에서 끝났을까요?\n머신러닝과 딥러닝 코드만으로는 실제 서비스를 운영할 수 없기 때문입니다. 실제 서비스 단계에서 머신러닝과 딥러닝의 코드가 차지하는 부분은 생각보다 작으며 단순히 모델만이 아니 다른 많은 부분들을 고려해야 합니다.\n구글은 이런 문제를 2015년 Hidden Technical Debt in Machine Learning Systems에서 지적한 바 있습니다.\n하지만 이 논문이 나올 때는 아직 많은 머신러닝 엔지니어들이 딥러닝과 머신러닝의 가능성을 입증하기 바쁜 시기였기 때문에, 논문이 지적하는 바에 많은 주의를 기울이지는 않았습니다.\n몇 년이 지난 후 머신러닝과 딥러닝은 가능성을 입증을 해냈고 사람들은 이제 실제 서비스에 적용하고자 했습니다.\n그리고 많은 사람들이 실제 서비스는 쉽지 않다는 것을 깨달았습니다.\n Before MLOps # 사람들은 우선 머신러닝과 딥러닝을 일반적인 소프트웨어와 동일한 시선으로 바라보고 기존과 같은 방식으로 운영하고자 했습니다.\n일반적인 소프트웨어에서는 어떤 문제가 발생할 경우 해당 부분을 담당한 소프트웨어 엔지니어가 문제의 원인을 진단하고 이를 해결한 후 다시 배포를 하였습니다.\n이 때 머신러닝 모델을 배포하기 위해서는 머신러닝 엔지니어가 직접 모델을 학습한 후 모델 파일들을 배포를 담당하는 소프트웨어 엔지니어에게 전달하였습니다. 두 직군간의 소통의 매개체는 \u0026lsquo;학습된 모델\u0026rsquo;이였습니다.\n하지만 이 소통의 매개체로 인해 머신러닝 엔지니어와 소프트웨어 엔지니어들의 갈등이 시작되었습니다.\n머신러닝 엔지니어와 소프트웨어 엔지니어들은 모델(Network 구조와 Weights 가 담긴 파일) 을 매개체로 서로 소통했습니다.\n머신러닝 엔지니어들은 DB에서 직접 쿼리를 이용해 데이터를 다운로드 받고 모델을 학습 후, 학습된 모델을 소프트웨어 엔지니어에게 전달하였습니다.\n그러면 소프트웨어 엔지니어는 전달받은 모델을 로드한 뒤, 정해진 추론(inference) 함수를 제공하는 API Server 를 만들어 배포하였습니다.\n이 과정에서 소프트웨어 엔지니어는 머신러닝 엔지니어에게 정해진 형식에 맞춰서 구현할 것을 요청합니다. 예를 들면 OS, 파이썬 버전, 사용한 패키지, 클래스 구조 등을 포함합니다.\n소프트웨어 엔지니어와 머신러닝 엔지니어는 서로 어떤 환경에서 작업을 하는지 알지 못하기 때문에 소통의 과정에서 미리 약속한 형식에서 하나라도 어긋난다면 배포와 성능 재현의 문제가 발생합니다.\n예를 들어서 개발 환경에서는 동작했던 코드가 실행 환경에서는 동작하지 않는다던지, 개발 환경에서과 동일한 성능이 재현되지 않는 문제가 발생하게 됩니다.\n또한 Data Shift와 같이 학습에 사용된 데이터와 다른 데이터가 들어왔을 때 모델의 성능이 감소하는 문제가 생겼습니다.\n일반적인 소프트웨어에서 문제를 해결하는 방법과 비슷하게 모델을 담당한 머신러닝 엔지니어들이 모델 성능 하락의 원인을 찾고 이를 개선한 새로운 모델을 배포했습니다.\n일반적인 경우 최신 데이터를 추가적으로 반영하여 모델을 재학습하는 것만으로도 모델의 성능의 하락을 개선할 수 있었기 때문에, 머신러닝 엔지니어들은 주기적으로 모델을 재학습해서 배포하였습니다.\nAfter MLOps # MLOps에서는 파이프라인을 이용해 서로 소통합니다. 여기서 파이프라인이란 단순히 학습된 모델만이 아닌 모델에 종속된 모든 작업을 포함한 개념입니다.\nContinuous Integration \u0026amp; Deployment # 머신러닝 엔지니어는 데이터를 다운로드 받고, 전처리를 수행하며, 모델을 생성하기까지의 모든 과정을 파이프라인의 형태로 작성하여 소프트웨어 엔지니어에게 전달하고, 소프트웨어 엔지니어는 전달받은 파이프라인에서 생성된 모델을 배포합니다.\n머신러닝 엔지니어와 소프트웨어 엔지니어는 같은 파이프라인을 사용하기 때문에 언제 어디서나 동일한 성능의 모델을 빌드하고 배포하는 것을 보장할 수 있게 됩니다.\nContinuous Training # 머신러닝 모델은 녹이 습니다. 이 말인 즉슨 한 번 학습되어 뛰어난 성능을 보였던 모델이라고 하더라도, 시간이 지남에 따라 성능이 저하되는 현상이 있다는 말입니다. 모델의 성능이 저하되는 근본적인 이유는 데이터의 분포가 변화하기 때문입니다.\n데이터의 분포는 왜 변할까요? 데이터의 분포가 변화하는 이유로는 데이터의 모분포를 모르기 때문일수도 있고 현실에서 얻어지는 데이터의 특성은 변화할 수 있기 때문입니다.\n이 때문에 최신 데이터를 이용해 모델을 재학습하여 모델의 성능을 다시 끌어올리는 작업이 필요하게 되었고, 머신러닝 엔지니어가 항상 이 작업을 반복하기보다는 파이프라인을 통해서 자동화를 할 수 있습니다.\n"}).add({id:3,href:"/docs/introduction/component/",title:"MLOps의 구성 요소",description:"Describe MLOps Components",content:"Practitioners guide to MLOps # 2021년 5월에 발표된 구글의 white paper : Practitioners guide to MLOps: A framework for continuous delivery and authmation of machine learning에서는 MLOps의 핵심 기능들로 다음과 같은 것들을 언급하였습니다.\n 각 기능들이 어떤 역할을 하는지 살펴보겠습니다.\n1. Experimentation # 실험(Experimentation)은 머신러닝 엔지니어들이 데이터를 분석하고, 프로토타입 모델을 만들며 학습 기능을 구현할 수 있도록 하는 다음과 같은 기능을 제공합니다.\n 깃(Git)과 같은 버전 컨트롤 툴과 통합된 노트북(Jupyter Notebook) 환경 제공 사용한 데이터, 하이퍼 파라미터, 평가 지표를 포함한 실험 추적 기능 제공 데이터와 모델에 대한 분석 및 시각화 기능 제공  2. Data Processing # 데이터 처리(Data Processing)은 머신러닝 모델 개발 단계, 지속적인 학습(Continuous Training) 단계, 그리고 API 배포(API Deployment) 단계에서 많은 양의 데이터를 사용할 수 있게 해 주는 다음과 같은 기능을 제공합니다.\n 다양한 데이터 소스와 서비스에 호환되는 데이터 커넥터(connector) 기능 제공 다양한 형태의 데이터와 호환되는 데이터 인코터(encoder) \u0026amp; 디코더(decoder) 기능 제공 다양한 형태의 데이터에 대한 데이터 변환과 피쳐 엔지니어링(feature engineering) 기능 제공 학습과 서빙을 위한 확장가능한 배치, 스트림 데이터 처리 기능 제공  3. Model training # 모델 학습(Model training)은 모델 학습을 위한 알고리즘을 효율적으로 실행시켜주는 다음과 같은 기능을 제공합니다.\n ML 프레임워크의 실행을 위한 환경 제공 다수의 GPU / 분산 학습 사용을 위한 분산 학습 환경 제공 하이퍼파라미터 튜닝과 최적화 기능 제공  4. Model evaluation # 모델 평가(Model evaluation)은 실험 환경과 상용 환경에서 동작하는 모델의 성능을 관찰할 수 있는 다음과 같은 기능을 제공합니다.\n 평가 데이터에 대한 모델 성능 평가 기능 서로 다른 지속 학습 실행결과에 대한 예측 성능 추적 서로 다른 모델의 성능 비교와 시각화 해석 가능한 AI 기술을 이용한 모델 출력 해석 기능 제공  5. Model serving # 모델 서빙(Model serving)은 상용 환경에 모델을 배포하고 서빙하기 위한 다음과 같은 기능들을 제공합니다.\n 저지연 추론과 고가용성 추론 기능 제공 다양한 ML 모델 서빙 프레임워크 지원(Tensorflow Serving, TorchServe, Nvidia Triton, Scikit-learn, XGGoost .. etc) 복잡한 형태의 추론 루틴 기능 제공, 예를 들어 전처리(preprocess) 또는 후처리(postprocess) 기능과 최종 결과를 위해 다수의 모델이 사용되는 경우를 말한다. 순간적으로 치솟는 추론 요청을 처리하기 위한 오토 스케일링(autoscaling) 기능 제공 추론 요청과 추론 결과에 대한 로깅 기능 제공  6. Online experimentation # 온라인 실험(Online experimentation)은 새로운 모델이 생성되었을 때, 이 모델을 배포하면 어느 정도의 성능을 보일 것인지 검증하는 기능을 제공한다. 이 기능은 새 모델을 배포하는 것 까지 연동하기 위해 모델 저장소(Model Registry)와 연동되어야 한다.\n 카나리(canary) \u0026amp; 섀도우(shadow) 배포 기능 제공 A/B 테스트 기능 제공 멀티 암드 밴딧(Multi-armed bandit) 테스트 기능 제공  7. Model Monitoring # 모델 모니터링(Model Monitoring)은 상용 환경에 배포되어 있는 모델이 정상적으로 동작하고 있는지를 모니터링 하는 기능을 제공한다. 이 기능은 모델의 성능이 떨어져 업데이트가 필요한지에 대한 정보를 제공해준다.\n8. ML Pipeline # 머신러닝 파이프라인(ML Pipeline)은 상용 환경에서 복잡한 ML 학습과 추론 작업을 구성하고 제어하고 자동화 하기 위한 다음과 같은 기능을 제공한다.\n 다양한 이벤트를 소스를 통한 파이프라인 실행 기능 파이프라인 파라미터와 생성되는 산출물 관리를 위한 머신러닝 메타데이터 추적과 연동 기능 일반적인 머신러닝 작업을 위한 내장 컴포넌트 지원과 사용자가 직접 구현한 컴포넌트에 대한 지원 기능 서로 다른 실행 환경 제공 기능  9. Model Registry # 모델 저장소(Model Registry)는 머신러닝 모델의 생명주기(Lifecycle)을 중앙 저장소에서 관리할 수 있게 해 주는 기능을 제공합니다.\n 학습된 모델 그리고 배포된 모델에 대한 등록, 추적, 버저닝 기능 제공 배포를 위해 필요한 데이터와 런타임 패키지들에 대한 정보 저장 기능  10. Dataset and Feature Repository #  데이터에 대한 공유, 검색, 재사용 그리고 버전관리 기능 이벤트 스트리밍 및 온라인 추론 작업에 대한 실시간 처리 및 저지연 서빙 기능 사진, 텍스트, 테이블 형태의 데이터와 같은 다양한 형태의 데이터 지원 기능  11. ML Metadata and Artifact Tracking # MLOps의 각 단계에서 다양한 형태의 산출물들이 생성된다. ML 메타데이터는 이런 산출물들에 대한 정보를 의미한다. ML 메타데이터와 산출물 관리는 산출물의 위치, 타입, 속성, 그리고 관련된 실험(experiment)에 대한 정보를 관리하기 위해 다음과 같은 기능들을 제공해준다.\n ML 산출물에 대한 히스토리 관리 기능 실험과 파이프라인 파라미터 설정에 대한 추적, 공유 기능 ML 산출물에 대한 저장, 접근, 시각화, 다운로드 기능 제공 기타 다른 MLOps 기능과의 통합 기능 제공  "}).add({id:4,href:"/docs/introduction/why_kubernetes/",title:"Why Kuberntes?",description:"Reason for using k8s in MLOps",content:"MLOps \u0026amp; Kubernetes # 그렇다면 MLOps를 이야기할 때, 쿠버네티스(Kubernetes)라는 단어가 항상 함께 들리는 이유가 무엇일까요?\n성공적인 MLOps 시스템을 구축하기 위해서는 MLOps의 구성요소 에서 설명한 것처럼 다양한 구성 요소들이 필요하지만, 각각의 구성 요소들이 유기적으로 운영되기 위해서는 인프라 레벨에서 수많은 이슈들을 해결해야 합니다.\n간단하게는 수많은 머신러닝 모델의 학습 요청을 순차적으로 실행 하는 것, 다른 작업 공간에서도 동일한 실행 환경을 보장해야 하는 것, 배포된 서비스에 장애가 생겼을 때 빠르게 대응해야 하는 것 등의 이슈 등을 생각해볼 수 있습니다.\n여기서 컨테이너(Container)와 컨테이너 오케스트레이션 시스템(Container Orchestration System)의 필요성이 등장합니다.\n쿠버네티스와 같은 컨테이너 오케스트레이션 시스템을 도입하면 실행 환경의 격리와 관리를 효율적으로 수행할 수 있습니다. 컨테이너 오케스트레이션 시스템을 도입한다면, 머신러닝 모델을 개발하고 배포하는 과정에서 다수의 개발자가 소수의 클러스터를 공유하면서 \u0026lsquo;1번 클러스터 사용 중이신가요?\u0026rsquo;, \u0026lsquo;GPU 사용 중이던 제 프로세스 누가 죽였나요?\u0026rsquo;, \u0026lsquo;누가 클러스터에 x 패키지 업데이트 했나요?' 와 같은 상황을 방지할 수 있습니다.\nContainer # 그렇다면 컨테이너란 무엇일까요? 마이크로소프트에서는 컨테이너를 다음과 같이 정의하고 있습니다.\n 컨테이너란 : 애플리케이션의 표준화된 이식 가능한 패키징\n 그런데 왜 머신러닝에서 컨테이너가 필요할까요? 머신러닝 모델들은 운영체제나 Python 실행 환경, 패키지 버전 등에 따라 다르게 동작할 수 있습니다.\n이를 방지하기 위해서 머신러닝에 사용된 소스코드와 함께 종속적인 실행 환경 전체를 하나로 묶어서(패키징해서) 공유하고 실행하는 데 활용할 수 있는 기술이 컨테이너라이제이션(Containerization) 기술입니다. 이렇게 패키징된 형태를 컨테이너 이미지라고 부르며, 컨테이너 이미지를 공유함으로써 사용자들은 어떤 시스템에서든 동일한 실행 결과를 보장할 수 있게 됩니다.\n즉, 단순히 Jupyter Notebook 파일이나, 모델의 소스코드와 requirements.txt 파일을 공유하는 것이 아닌, 모든 실행 환경이 담긴 컨테이너 이미지를 공유한다면 \u0026ldquo;제 노트북에서는 잘 되는데요?\u0026quot; 와 같은 상황을 피할 수 있습니다.\n컨테이너를 처음 접하시는 분들이 흔히 하시는 오해 중 하나는 \u0026ldquo;컨테이너 == 도커\u0026ldquo;라고 받아들이는 것입니다.\n도커는 컨테이너와 동일한 의미를 지니는 개념이 아니라, 컨테이너를 띄우거나, 컨테이너 이미지를 만들고 공유하는 것과 같이 컨테이너를 보다 쉽고 유연하게 사용할 수 있는 기능을 제공해주는 도구입니다. 정리하자면 컨테이너는 가상화 기술이고, 도커는 가상화 기술의 구현체라고 말할 수 있습니다.\n다만, 도커는 여러 컨테이너 가상화 도구 중에서 쉬운 사용성과 높은 효율성을 바탕으로 가장 빠르게 성장하여 대세가 되었기에 컨테이너하면 도커라는 이미지가 자동으로 떠오르게 되었습니다. 이렇게 컨테이너와 도커 생태계가 대세가 되기까지는 다양한 이유가 있지만, 기술적으로 자세한 이야기는 모두의 MLOps의 범위를 넘어서기 때문에 다루지는 않겠습니다.\n컨테이너 혹은 도커를 처음 들어보시는 분들에게는 모두의 MLOps의 내용이 다소 어렵게 느껴질 수 있기 때문에, 생활코딩, subicura 님의 개인 블로그 글 등의 자료를 먼저 살펴보는 것을 권장합니다.\nContainer Orchestration System # 그렇다면 컨테이너 오케스트레이션 시스템은 무엇일까요? 오케스트레이션이라는 단어에서 추측해 볼 수 있듯이, 수많은 컨테이너들이 있을 때 컨테이너들이 서로 조화롭게 구동될 수 있도록 지휘하는 시스템에 비유할 수 있습니다.\n컨테이너를 도입이 되면 서비스는 컨테이너의 형태로 사용자들에게 제공됩니다. 이 때 관리해야 할 컨테이너의 수가 적다면 운영 담당자 한 명이서도 충분히 모든 상황에 대응할 수 있습니다.\n하지만, 수 백 개 이상의 컨테이너가 수 십 대 이상의 클러스터에서 구동되고 있고 장애를 일으키지 않고 항상 정상 동작해야 한다면, 모든 서비스의 정상 동작 여부를 담당자 한 명이 파악하고 이슈에 대응하는 것은 불가능에 가깝습니다.\n예를 들면, 모든 서비스가 정상적으로 동작하고 있는지를 계속해서 모니터링(Monitoring)해야 합니다.\n만약, 특정 서비스가 장애를 일으켰다면 여러 컨테이너들의 로그를 확인해가며 문제를 파악해야 합니다.\n또한 특정 클러스터나 특정 컨테이너에 작업이 몰리지 않도록 스케줄링(Scheduling)하고 로드 밸런싱(Load Balancing)하며, 스케일링(Scaling)하는 등의 수많은 작업을 담당해야 합니다. 이렇게 수많은 컨테이너들의 상태를 지속적으로 관리하고 운영하는 과정을 조금이나마 쉽게, 자동으로 할 수 있는 기능을 제공해주는 소프트웨어가 바로 컨테이너 오케스트레이션 시스템입니다.\n머신러닝에서는 어떻게 쓰일 수 있을까요?\n예를 들어서 GPU를 필요로 하는 딥러닝 학습 코드가 패키징된 컨테이너는 사용 가능한 GPU가 있는 클러스터에서 수행하고, 많은 메모리를 필요로 하는 데이터 전처리 코드가 패키징된 컨테이너는 메모리의 여유가 많은 클러스터에서 수행하고, 학습 중에 클러스터에 문제가 생기면 자동으로 동일한 컨테이너를 다른 클러스터로 이동시키고 다시 학습을 진행하는 등의 작업을 사람이 일일히 수행하지 않고, 자동으로 관리하는 시스템을 개발한 뒤 맡기는 것입니다.\n집필을 하는 2022년을 기준으로 쿠버네티스는 컨테이너 오케스트레이션 시스템의 사실상의 표준(De facto standard)입니다.\nCNCF에서 2018년 발표한 Survey 에 따르면 다음 그림과 같이 이미 두각을 나타내고 있었으며, 2019년 발표한 Survey에 따르면 그 중 78% 가 상용 수준(Production Level)에서 사용하고 있다는 것을 알 수 있습니다.\n쿠버네티스 생태계가 이처럼 커지게 된 이유에는 여러 가지 이유가 있습니다. 하지만 도커와 마찬가지로 쿠버네티스 역시 머신러닝 기반의 서비스에서만 사용하는 기술이 아니기에, 자세히 다루기에는 상당히 많은 양의 기술적인 내용을 다루어야 하므로 이번 모두의 MLOps에서는 자세한 내용은 생략할 예정입니다.\n다만, 모두의 MLOps에서 앞으로 다룰 내용은 도커와 쿠버네티스에 대한 내용을 어느 정도 알고 계신 분들을 대상으로 작성하였습니다. 따라서 쿠버네티스에 대해 익숙하지 않으신 분들은 다음 쿠버네티스 공식 문서, subicura 님의 개인 블로그 글 등의 쉽고 자세한 자료들을 먼저 참고해주시는 것을 권장합니다.\n"}).add({id:5,href:"/docs/setup/",title:"Setup",description:"Setup kubernetes.",content:""}).add({id:6,href:"/docs/setup/intro/",title:"1. Introduction",description:"Setup Introduction",content:"MLOps 시스템 구축해보기 # MLOps 를 공부하는 데 있어서 가장 큰 장벽은 MLOps 시스템을 구성해보고 사용해보기가 어렵다는 점입니다. AWS, GCP 등의 퍼블릭 클라우드 혹은 Weight \u0026amp; Bias, neptune.ai 등의 상용 툴을 사용해보기에는 과금에 대한 부담이 존재하고, 처음부터 모든 환경을 혼자서 구성하기에는 어디서부터 시작해야 할 지 막막하게 느껴질 수밖에 없습니다.\n이런 이유들로 MLOps 를 선뜻 시작해보지 못하시는 분들을 위해, 모두의 MLOps에서는 우분투가 설치되는 데스크탑 하나만 준비되어 있다면 MLOps 시스템을 밑바닥부터 구축하고 사용해 볼 수 있는 방법을 다룰 예정입니다.\n하지만 MLOps의 구성요소에서 설명하는 요소들을 모두 사용해볼 수는 없기에, 모두의 MLOps에서는 대표적인 오픈소스만을 설치한 뒤, 서로 연동하여 사용하는 부분을 주로 다룰 예정입니다.\n모두의 MLOps에서 설치하는 오픈소스가 표준을 의미하는 것은 아니며, 여러분의 상황에 맞게 적절한 툴을 취사 선택하시는 것을 권장합니다.\n구성 요소 # 이 글에서 만들어 볼 MLOps 시스템의 구성 요소들과 각 버전은 아래와 같은 환경에서 검증되었습니다.\n원활한 환경에서 테스트하기 위해 클러스터 (혹은 클러스터) 와 클라이언트를 분리하여 설명드릴 예정입니다.\n클러스터 는 우분투가 설치 되어 있는 데스크탑을 의미합니다.\n클라이언트 는 노트북 혹은 클러스터가 설치 되어 있는 데스크탑 외의 클라이언트로 사용할 수 있는 다른 데스크탑을 사용하는 것을 권장합니다.\n하지만 두 대의 머신을 준비할 수 없다면 데스크탑 하나를 동시에 클러스터와 클라이언트 용도로 사용하셔도 괜찮습니다.\n클러스터 # 1. Software # 아래는 클러스터에 설치해야할 소프트웨어 목록입니다.\n   Software Version     Ubuntu 20.04.3 LTS   Docker (Server) 20.10.11   Nvidia-Driver 470.86   Kubernetes v1.21.7   Kubeflow v1.4.0   MLFlow v1.21.0    2. Helm Chart # 아래는 Helm을 이용해 설치되어야 할 써드파티 소프트웨어 목록입니다.\n   Helm Chart Repo Name Version     datawire/ambassador v6.9.3   prometheus-community/kube-prometheus-stack v21.0.0    클라이언트 # 클라이언트는 MacOS (Intel CPU), Ubuntu 20.04 에서 검증되었습니다.\n   Software Version     kubectl v1.21.7   helm v3.7.1   kustomize v3.10.0    Minimum System Requirements # 모두의 MLOps 를 설치할 클러스터는 다음과 같은 사양을 만족시키는 것을 권장합니다.\n이는 Kubernetes 및 Kubeflow 의 권장 사양에 의존합니다.\n CPU : 6 core RAM : 12 GB DISK : 50 GB GPU : NVIDIA GPU (Optional)  "}).add({id:7,href:"/docs/setup/kubernetes/",title:"2. Setup Kubernetes",description:"Setup Kubernetes",content:"Setup Kubernetes Cluster # 쿠버네티스를 처음 배우시는 분들에게 첫 진입 장벽은 쿠버네티스 실습 환경을 구축하는 것입니다.\n프로덕션 레벨의 쿠버네티스 클러스터를 구축할 수 있게 공식적으로 지원하는 도구는 kubeadm 이지만, 사용자들이 조금 더 쉽게 구축할 수 있도록 도와주는 kubespray, kops 등의 도구도 존재하며, 학습 목적을 위해서 컴팩트한 쿠버네티스 클러스터를 정말 쉽게 구축할 수 있도록 도와주는 k3s, minikube, microk8s, kind 등의 도구도 존재합니다.\n각각의 도구는 장단점이 다르기에 사용자마다 선호하는 도구가 다른 점을 고려하여, 본 글에서는 kubeadm, k3s, minikube 의 3 가지 도구를 활용하여 쿠버네티스 클러스터를 구축하는 방법을 다룹니다. 각 도구에 대한 자세한 비교는 다음 쿠버네티스 공식 문서를 확인해주시기 바랍니다.\n본 모두의 MLOps에서는 구축하게 될 MLOps 구성 요소들을 원활히 사용하기 위해, 각각의 도구를 활용해 쿠버네티스 클러스터를 구축할 때, 추가적으로 설정해주어야 하는 부분이 추가되어 있습니다.\nUbuntu OS 까지는 설치되어 있는 데스크탑을 k8s cluster 로 구축한 뒤, 외부 클라이언트 노드에서 쿠버네티스 클러스터에 접근하는 것을 확인하는 것까지가 본 Setup Kubernetes단원의 범위입니다.\n자세한 구축 방법은 3 가지 도구마다 다르기에 다음과 같은 흐름으로 구성되어 있습니다.\n3. Setup Prerequisite 4. Setup Kubernetes 4.1. with kubeadm 4.2. with minikube 4.3. with k3s 5. Setup Kubernetes Modules 그럼 이제 각각의 도구를 활용해 쿠버네티스 클러스터를 구축해보겠습니다. 반드시 모든 도구를 사용해 볼 필요는 없으며, 이 중 여러분이 익숙하신 도구를 활용해주시면 충분합니다.\n"}).add({id:8,href:"/docs/setup/setup-pre-requisite/",title:"3. Setup Prerequisite",description:"Install docker",content:"이 페이지에서는 쿠버네티스를 설치하기에 앞서, 클러스터와 클라이언트에 설치 혹은 설정해두어야 하는 컴포넌트들에 대한 매뉴얼을 설명합니다.\nInstall apt packages # 추후 클라이언트와 클러스터의 원활한 통신을 위해서는 Port-Forwarding 을 수행해야 할 일이 있습니다. Port-forwarding 을 위해서는 클러스터에 다음 패키지를 설치해주어야 합니다.\nsudo apt-get update sudo apt-get install -y socat Install Docker #   도커 설치에 필요한 APT 패키지들을 설치합니다.\nsudo apt-get update \u0026amp;\u0026amp; sudo apt-get install ca-certificates curl gnupg lsb-release   도커의 공식 GPG key 를 추가합니다.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg   apt 패키지 매니저로 도커를 설치할 때, stable Repository 에서 받아오도록 설정합니다.\necho \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null   현재 설치 가능한 도커 버전을 확인합니다.\napt-cache madison docker-ce   5:20.10.11~3-0~ubuntu-focal 버전이 있는지 확인하고, 해당 버전의 도커를 설치합니다.\nsudo apt-get install containerd.io docker-ce=5:20.10.11~3-0~ubuntu-focal docker-ce-cli=5:20.10.11~3-0~ubuntu-focal   도커가 정상적으로 설치된 것을 확인합니다.\nsudo docker run hello-world 명령어 실행 후 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ sudo docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   docker 관련 command를 sudo 키워드 없이 사용할 수 있도록 하기 위해 다음 명령어를 통해 권한을 추가합니다.\nsudo groupadd docker sudo usermod -aG docker $USER newgrp docker   sudo 키워드 없이 docker command를 사용할 수 있게 된 것을 확인하기 위해, 다시 한 번 docker run을 실행합니다.\nmlops@ubuntu:~$ docker run hello-world Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \u0026#34;hello-world\u0026#34; image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/   Turn off Swap Memory # kubelet 이 정상적으로 동작하게 하기 위해서는 클러스터 노에 swap이라고 불리는 가상메모리를 꺼 두어야 합니다. 다음 명령어를 통해 swap을 꺼 둡니다.\n(클러스터와 클라이언트를 동일한 데스크탑에서 사용할 때 swap 메모리를 종료할 경우 속도의 저하가 있을 수 있습니다.)\nsudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab sudo swapoff -a Install Kubectl # kubectl 은 쿠버네티스 클러스터에게 API 를 요청할 때 사용하는 클라이언트 툴 입니다. 클라이언트 노드에 설치해두어야 합니다.\n  현재 폴더에 kubectl v1.21.7 버전을 다운받습니다.\ncurl -LO https://dl.k8s.io/release/v1.21.7/bin/linux/amd64/kubectl   kubectl 을 사용할 수 있도록 파일의 권한과 위치를 변경합니다.\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl   정상적으로 설치되었는지 확인합니다.\nkubectl --help 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nkubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ Basic Commands (Beginner): create Create a resource from a file or from stdin expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes service run Run a particular image on the cluster set Set specific features on objects ...   여러 개의 쿠버네티스 클러스터를 사용하는 경우, 여러 개의 kubeconfig 파일을 관리해야 하는 경우가 있습니다.\n여러 개의 kubeconfig 파일 혹은 여러 개의 kube-context 를 효율적으로 관리하는 방법은 다음과 같은 문서를 참고하시기 바랍니다.\n https://dev.to/aabiseverywhere/configuring-multiple-kubeconfig-on-your-machine-59eo https://github.com/ahmetb/kubectx)    References #  Install Docker Engine on Ubuntu 리눅스에 kubectl 설치 및 설정  "}).add({id:9,href:"/docs/setup/kubernetes-with-kubeadm/",title:"4.1. Setup Kubernetes - Kubeadm",description:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성요소들을 클러스터에 설치합니다.\nSetup Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\n쿠버네티스를 위한 네트워크의 설정을 변경합니다.\nsudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 2. 쿠버네티스 클러스터 셋업 # $ sudo modprobe br_netfilter $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.",content:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성요소들을 클러스터에 설치합니다.\nSetup Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\n쿠버네티스를 위한 네트워크의 설정을 변경합니다.\nsudo modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system 2. 쿠버네티스 클러스터 셋업 # $ sudo modprobe br_netfilter $ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF - kubeadm : kubelet을 서비스에 등록하고, 클러스터 컴포넌트들 사이의 통신을 위한 인증서 발급 등 설치 과정 자동화 - kubelet : container 리소스를 실행, 종료를 해 주는 컨테이너 핸들러 - kubectl : 쿠버네티스 클러스터를 터미널에서 확인, 조작 하기 위한 CLI 툴 다음 명령어를 통해 kubeadm, kubelet, kubectl을 설치합니다. 실수로 이 컴포넌트들의 버전이 변경할 경우, 예기치 않은 장애를 낳을 수 있으므로 컴포넌트들이 변경되지 않도록 설정합니다. ```text sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \u0026quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubelet=1.21.7-00 kubeadm=1.21.7-00 kubectl=1.21.7-00 sudo apt-mark hold kubelet kubeadm kubectl kubeadm, kubelet, kubectl 이 잘 설치되었는지 확인합니다.\nmlops@ubuntu:~$ kubeadm ┌──────────────────────────────────────────────────────────┐ │ KUBEADM │ │ Easily bootstrap a secure Kubernetes cluster │ │ │ │ Please give us feedback at: │ │ https://github.com/kubernetes/kubeadm/issues │ └──────────────────────────────────────────────────────────┘ ... mlops@ubuntu:~$ kubelet -h The kubelet is the primary \u0026#34;node agent\u0026#34; that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider. ... mlops@ubuntu:~$ kubectl kubectl controls the Kubernetes cluster manager. Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/ Basic Commands (Beginner): create Create a resource from a file or from stdin. expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service run Run a particular image on the cluster set Set specific features on objects ... 이제 kubeadm을 사용하여 쿠버네티스를 설치합니다.\nkubeadm config images list kubeadm config images pull sudo kubeadm init --pod-network-cidr=10.244.0.0/16 kubectl을 통해서 쿠버네티스 클러스터를 제어할 수 있도록 admin 인증서를 $HOME/.kube/config 경로에 복사합니다.\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config CNI를 설치합니다. 쿠버네티스 내부의 네트워크 설정을 전담하는 CNI는 여러 종류가 있으며, 모두의 MLOps에서는 flannel을 사용합니다.\nkubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.13.0/Documentation/kube-flannel.yml 쿠버네티스 노드의 종류에는 크게 마스터 노드와 워커 노드가 있습니다. 안정성을 위하여 마스터 노드에는 쿠버네티스 클러스터를 제어하는 작업들만 실행되도록 하는 것이 일반적이지만, 이 매뉴얼에서는 싱글 클러스터를 가정하고 있으므로 마스터 노드에 모든 종류의 작업이 실행될 수 있도록 설정합니다.\nkubectl taint nodes --all node-role.kubernetes.io/master- 3. 쿠버네티스 클라이언트 셋업 # 클러스터에 생성된 kubeconfig 파일을 클라이언트에 복사하여 kubectl을 통해 클러스터를 제어할 수 있도록 합니다.\nmkdir -p $HOME/.kube scp -p {CLUSTER_USER_ID}@{CLUSTER_IP}:~/.kube/config ~/.kube/config 4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 다음 명령어를 통해 노드의 STATUS가 Ready 상태가 되었는지 확인합니다.\nkubectl get nodes NAME STATUS ROLES AGE VERSION ubuntu Ready control-plane,master 2m55s v1.21.7 6. References #  kubeadm  "}).add({id:10,href:"/docs/setup/kubernetes-with-minikube/",title:"4.2. Setup Kubernetes - Minikube",description:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성요소들을 클러스터에 설치합니다.\nSetup Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nMinikube binary # Minikube 를 사용하기 위해, v1.24.0 버전의 Minikube 바이너리를 설치합니다.\nwget https://github.com/kubernetes/minikube/releases/download/v1.24.0/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube 정상적으로 설치되었는지 확인합니다.\nminikube version 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ minikube version minikube version: v1.24.0 commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b 2. 쿠버네티스 클러스터 셋업 # 이제 Minikube 를 활용해 쿠버네티스 클러스터를 클러스터에 구축합니다.",content:"1. Prerequisite # 쿠버네티스 클러스터를 구축하기에 앞서, 필요한 구성요소들을 클러스터에 설치합니다.\nSetup Prerequisite을 참고하여 Kubernetes를 설치하기 전에 필요한 요소들을 클러스터에 설치해 주시기 바랍니다.\nMinikube binary # Minikube 를 사용하기 위해, v1.24.0 버전의 Minikube 바이너리를 설치합니다.\nwget https://github.com/kubernetes/minikube/releases/download/v1.24.0/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube 정상적으로 설치되었는지 확인합니다.\nminikube version 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ minikube version minikube version: v1.24.0 commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b 2. 쿠버네티스 클러스터 셋업 # 이제 Minikube 를 활용해 쿠버네티스 클러스터를 클러스터에 구축합니다. GPU 의 원활한 사용과, 클러스터-클라이언트 간 통신을 간편하게 수행하기 위해, Minikube 는 driver=none 옵션을 활용하여 실행합니다. driver=none 옵션은 root user 로 실행해야함에 주의바랍니다.\nroot user 로 전환합니다.\nsudo su minikube start 를 수행하여 쿠버네티스 클러스터 구축을 진행합니다. Kubeflow 의 원활한 사용을 위해, 쿠버네티스 버전은 v1.21.7 로 지정하여 구축하며 --extra-config 를 추가합니다.\nminikube start --driver=none \\ --kubernetes-version=v1.21.7 \\ --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \\ --extra-config=apiserver.service-account-issuer=kubernetes.default.svc Disable default addons # Minikube 를 설치하면 Default 로 설치되는 addon 이 존재합니다. 이 중 저희가 사용하지 않을 addon 을 비활성화합니다.\nminikube addons disable storage-provisioner minikube addons disable default-storageclass 모든 addon 이 비활성화된 것을 확인합니다.\nminikube addons list 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nroot@ubuntu:/home/mlops# minikube addons list |-----------------------------|----------|--------------|-----------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|-----------------------| | ambassador | minikube | disabled | unknown (third-party) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | disabled | kubernetes | | efk | minikube | disabled | unknown (third-party) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | unknown (third-party) | | ingress | minikube | disabled | unknown (third-party) | | ingress-dns | minikube | disabled | unknown (third-party) | | istio | minikube | disabled | unknown (third-party) | | istio-provisioner | minikube | disabled | unknown (third-party) | | kubevirt | minikube | disabled | unknown (third-party) | | logviewer | minikube | disabled | google | | metallb | minikube | disabled | unknown (third-party) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | unknown (third-party) | | olm | minikube | disabled | unknown (third-party) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | unknown (third-party) | | storage-provisioner | minikube | disabled | kubernetes | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|-----------------------| 3. 쿠버네티스 클라이언트 셋업 # 이번에는 클라이언트에 쿠버네티스의 원활한 사용을 위한 도구를 설치합니다. 클라이언트와 클러스터 노드가 분리되지 않은 경우에는 root user 로 모든 작업을 진행해야 함에 주의바랍니다.\n클라이언트와 클러스터 노드가 분리된 경우, 우선 kubernetes 의 관리자 인증 정보를 클라이언트로 가져옵니다.\n 클러스터에서 config를 확인합니다.  # 클러스터 노드 minikube kubectl -- config view --flatten 다음과 같은 정보가 출력됩니다.  apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUd.... extensions: - extension: last-update: Mon, 06 Dec 2021 06:55:46 UTC provider: minikube.sigs.k8s.io version: v1.24.0 name: cluster_info server: https://192.168.0.62:8443 name: minikube contexts: - context: cluster: minikube extensions: - extension: last-update: Mon, 06 Dec 2021 06:55:46 UTC provider: minikube.sigs.k8s.io version: v1.24.0 name: context_info namespace: default user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate-data: LS0tLS1CRUdJTi.... client-key-data: LS0tLS1CRUdJTiBSU0.... 클라이언트 노드에서 .kube 폴더를 생성합니다.  # 클라이언트 노드 mkdir -p /home/$USER/.kube 해당 파일에 2. 에서 출력된 정보를 붙여넣은 뒤 저장합니다.  vi /home/$USER/.kube/config 4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 최종적으로 node 가 Ready 인지, OS, Docker, Kubernetes 버전을 확인합니다.\nkubectl get nodes -o wide 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ubuntu Ready control-plane,master 2d23h v1.21.7 192.168.0.75 \u0026lt;none\u0026gt; Ubuntu 20.04.3 LTS 5.4.0-91-generic docker://20.10.11 "}).add({id:11,href:"/docs/setup/kubernetes-with-k3s/",title:"4.3. Setup Kubernetes - K3s",description:"해당 과정은 클러스터로 사용하는 데스크탑에서 진행됩니다. 로컬과 클러스터가 분리된 경우 꼭 클러스터에서 설치되도록 확인해 주세요.\n1. Prerequisite # k3s 에서는 기본값으로 containerd를 백엔드로 이용해 설치합니다. 하지만 저희는 GPU를 사용하기 위해서 docker를 백엔드로 사용해야 하기 때문에 --docker 옵션을 통해 백엔드를 docker로 설치하겠습니다.\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.21.7+k3s1 sh -s - server --disable traefik --disable servicelb --disable local-storage --docker k3s를 설치후 k3s config를 확인합니다\ncat /etc/rancher/k3s/k3s.yaml 2. 쿠버네티스 클러스터 셋업 # k3s config를 클러스터의 kubeconfig로 사용하기 위해서 복사합니다.",content:"해당 과정은 클러스터로 사용하는 데스크탑에서 진행됩니다. 로컬과 클러스터가 분리된 경우 꼭 클러스터에서 설치되도록 확인해 주세요.\n1. Prerequisite # k3s 에서는 기본값으로 containerd를 백엔드로 이용해 설치합니다. 하지만 저희는 GPU를 사용하기 위해서 docker를 백엔드로 사용해야 하기 때문에 --docker 옵션을 통해 백엔드를 docker로 설치하겠습니다.\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.21.7+k3s1 sh -s - server --disable traefik --disable servicelb --disable local-storage --docker k3s를 설치후 k3s config를 확인합니다\ncat /etc/rancher/k3s/k3s.yaml 2. 쿠버네티스 클러스터 셋업 # k3s config를 클러스터의 kubeconfig로 사용하기 위해서 복사합니다.\nmkdir .kube sudo cp /etc/rancher/k3s/k3s.yaml .kube/config sudo chown mrx:mrx .kube/config 3. 쿠버네티스 클라이언트 셋업 # 이제 클러스터에서 설정한 kubeconfig를 로컬로 이동합니다. 로컬에서는 경로를 ~/.kube/config로 설정합니다. 정상적으로 작동하는지 확인합니다.\n4. 쿠버네티스 기본 모듈 설치 # Setup Kubernetes Modules을 참고하여 다음 컴포넌트들을 설치해 주시기 바랍니다.\n helm kustomize CSI plugin [Optional] nvidia-docker, nvidia-device-plugin  5. 정상 설치 확인 # 최종적으로 node 가 Ready 인지, OS, Docker, Kubernetes 버전을 확인합니다.\nkubectl get nodes -o wide 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ubuntu Ready control-plane,master 2d23h v1.21.7 192.168.0.75 \u0026lt;none\u0026gt; Ubuntu 20.04.3 LTS 5.4.0-91-generic docker://20.10.11 6. References #  https://rancher.com/docs/k3s/latest/en/installation/install-options/  "}).add({id:12,href:"/docs/setup/setup-kubernetes-module/",title:"5. Setup Kubernetes Modules",description:"Install Helm, Kustomize",content:"Helm # Helm 은 쿠버네티스 패키지와 관련된 리소스를 한 번에 배포하고 관리할 수 있게 도와주는 패키지 매니징 도구 중 하나입니다.\n 현재 폴더에 Helm v3.7.1 버전을 다운받습니다.  wget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz helm 을 사용할 수 있도록 압축을 풀고, 파일의 위치를 변경합니다.  tar -zxvf helm-v3.5.4-linux-amd64.tar.gz sudo mv linux-amd64/helm /usr/local/bin/helm 정상적으로 설치되었는지 확인합니다.  helm help 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nThe Kubernetes package manager Common actions for Helm: - helm search: search for charts - helm pull: download a chart to your local directory to view - helm install: upload the chart to Kubernetes - helm list: list releases of charts Environment variables: | Name | Description | |--------------------------|---------------------------------------------------------------------| | $HELM_CACHE_HOME | set an alternative location for storing cached files. | | $HELM_CONFIG_HOME | set an alternative location for storing Helm configuration. | | $HELM_DATA_HOME | set an alternative location for storing Helm data. | ... Kustomize # kustomize 또한 여러 쿠버네티스 리소스를 한 번에 배포하고 관리할 수 있게 도와주는 패키지 매니징 도구 중 하나입니다.\n 현재 폴더에 kustomize v3.10.0 버전을 다운받습니다.  wget https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv3.10.0/kustomize_v3.10.0_linux_amd64.tar.gz kustomize 를 사용할 수 있도록 압축을 풀고, 파일의 위치를 변경합니다.  tar -zxvf kustomize_v3.10.0_linux_amd64.tar.gz sudo mv kustomize_3.2.0_linux_amd64 /usr/local/bin/kustomize 정상적으로 설치되었는지 확인합니다.  kustomize help 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nManages declarative configuration of Kubernetes. See https://sigs.k8s.io/kustomize Usage: kustomize [command] Available Commands: build Print configuration per contents of kustomization.yaml cfg Commands for reading and writing configuration. completion Generate shell completion script create Create a new kustomization in the current directory edit Edits a kustomization file fn Commands for running functions against configuration. ... CSI Plugin : Local Path Provisioner #  CSI Plugin 은 kubernetes 내의 스토리지를 담당하는 모듈입니다. 단일 노드 클러스터에서 쉽게 사용할 수 있는 CSI Plugin 인 Local Path Provisioner 를 설치합니다.  kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.20/deploy/local-path-storage.yaml 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nnamespace/local-path-storage created serviceaccount/local-path-provisioner-service-account created clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created deployment.apps/local-path-provisioner created storageclass.storage.k8s.io/local-path created configmap/local-path-config created 또한, 다음과 같이 local-path-storage namespace 에 provisioner pod 가 Running 인지 확인합니다.  kubectl -n local-path-storage get pod 정상적으로 수행할 경우 아래와 같이 출력됩니다.\nNAME READY STATUS RESTARTS AGE local-path-provisioner-d744ccf98-xfcbk 1/1 Running 0 7m 다음을 수행하여 default storage class 로 변경합니다.  kubectl patch storageclass local-path -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;:{\u0026#34;storageclass.kubernetes.io/is-default-class\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; default storage class 로 설정되었는지 확인합니다.  kubectl get sc 다음과 같이 NAME 에 local-path (default) 인 storage class 가 존재하는 것을 확인합니다.\nNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 2h "}).add({id:13,href:"/docs/setup/setup-nvidia-gpu/",title:"6. (Optional) Setup GPU",description:"Install nvidia docker, nvidia device plugin",content:"쿠버네티스 및 Kubeflow 등에서 GPU 를 사용하기 위해서는 다음 작업이 필요합니다.\n1. Install NVIDIA Driver # nvidia-smi 수행 시 다음과 같은 화면이 출력된다면 이 단계는 스킵해 주시기 바랍니다.\nmlops@ubuntu:~$ nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | 25% 32C P8 4W / 120W | 211MiB / 6078MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:02:00.0 Off | N/A | | 0% 34C P8 7W / 175W | 5MiB / 7982MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1644 G /usr/lib/xorg/Xorg 198MiB | | 0 N/A N/A 1893 G /usr/bin/gnome-shell 10MiB | | 1 N/A N/A 1644 G /usr/lib/xorg/Xorg 4MiB | +-----------------------------------------------------------------------------+ nvidia-smi의 출력 결과가 위와 같지 않다면 장착되어 있는 GPU에 맞는 nvidia driver를 설치해 주시기 바랍니다.\n만약 nvidia driver의 설치에 익숙하지 않다면 아래 명령어를 통해 설치하시기 바랍니다.\nsudo add-apt-repository ppa:graphics-drivers/ppa sudo apt update \u0026amp;\u0026amp; sudo apt install -y ubuntu-drivers-common sudo ubuntu-drivers autoinstall sudo reboot 2. NVIDIA-Docker 설치 # NVIDIA-Docker 를 설치합니다.\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update sudo apt-get install -y nvidia-docker2 sudo systemctl restart docker 정상적으로 설치되었는지 확인하기 위해, GPU 를 사용하는 도커 컨테이너를 실행해봅니다.\nsudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.86 Driver Version: 470.86 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:01:00.0 Off | N/A | | 25% 32C P8 4W / 120W | 211MiB / 6078MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:02:00.0 Off | N/A | | 0% 34C P8 6W / 175W | 5MiB / 7982MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +-----------------------------------------------------------------------------+ 3. NVIDIA-Docker 를 Default Container Runtime 으로 설정 # 쿠버네티스는 기본적으로 Docker-CE 를 Default Container Runtime 으로 사용합니다. 따라서, Docker Container 내에서 NVIDIA GPU 를 사용하기 위해서는 NVIDIA-Docker 를 Container Runtime 으로 사용하여 pod 를 생성할 수 있도록 Default Runtime 을 수정해주어야 합니다.\n /etc/docker/daemon.json 파일을 열어 다음과 같이 수정합니다.  sudo vi /etc/docker/daemon.json { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } 파일이 변경된 것을 확인한 후, Docker 를 재시작합니다.  sudo systemctl daemon-reload sudo service docker restart 변경 사항이 반영되었는지 확인합니다.  sudo docker info | grep nvidia 다음과 같은 메시지가 보이면 정상적으로 설치된 것을 의미합니다.\nmlops@ubuntu:~$ docker info | grep nvidia Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc Default Runtime: nvidia 4. Nvidia-Device-Plugin #  nvidia-device-plugin daemonset을 생성합니다.  kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.10.0/nvidia-device-plugin.yml nvidia-device-plugin pod이 RUNNING 상태로 생성되었는지 확인합니다.  kubectl get pod -n kube-system | grep nvidia 다음과 같은 결과가 출력되어야 합니다.\nkube-system nvidia-device-plugin-daemonset-nlqh2 1/1 Running 0 1h node 정보에 gpu 가 사용가능하도록 설정되었는지 확인합니다.  kubectl get nodes \u0026#34;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\u0026#34; 다음과 같은 메시지가 보이면 정상적으로 설정된 것을 의미합니다.\n(모두의 MLOps 에서 실습을 진행한 클러스터는 2개의 GPU가 있기 때문에 2가 출력이 됩니다. 본인의 클러스터의 GPU 개수와 맞는 숫자가 출력된다면 됩니다.)\nNAME GPU ubuntu 2 설정되지 않은 경우, GPU 의 value 가 \u0026lt;None\u0026gt; 으로 표시됩니다.\n"}).add({id:14,href:"/docs/kubeflow/",title:"Kubeflow",description:"How to use Kubeflow.",content:""}).add({id:15,href:"/docs/kubeflow/example/",title:"Kubeflow example",description:"Introduction to MLOps",content:"예시 페이지 입니다. 헤딩은 2번 부터 시작해주세요. # "}).add({id:16,href:"/docs/api-deployment/",title:"API Deployment",description:"API deployment with seldon-core",content:""}).add({id:17,href:"/docs/api-deployment/example/",title:"API Deployment example",description:"Introduction to MLOps",content:"예시 페이지 입니다. 헤딩은 2번 부터 시작해주세요. # "}).add({id:18,href:"/docs/help/",title:"Help",description:"Help Doks.",content:""}).add({id:19,href:"/docs/help/how-to-contribute/",title:"How to Contribute",description:"How to Start #  필요한 node module을 설치합니다.  npm install 글 수정 및 추가를 후 ci 를 실행합니다.  npm ci node 클러스터를 실행 후 수정한 글이 정상적으로 나오는지 확인합니다.  npm run start How to Contribute # 1. 새로운 포스트를 작성하는 경우 # 새로운 포스트는 각 챕터와 포스트의 위치에 맞는 weight를 설정합니다.\n Introduction: 1xx Setup: 2xx Kubeflow: 3xx API Deployment: 4xx Help: 10xx  2. 기존의 포스트를 수정하는 경우 # 기존의 포스트를 수정할 경우 Contributor에 본인의 이름을 입력합니다.",content:"How to Start #  필요한 node module을 설치합니다.  npm install 글 수정 및 추가를 후 ci 를 실행합니다.  npm ci node 클러스터를 실행 후 수정한 글이 정상적으로 나오는지 확인합니다.  npm run start How to Contribute # 1. 새로운 포스트를 작성하는 경우 # 새로운 포스트는 각 챕터와 포스트의 위치에 맞는 weight를 설정합니다.\n Introduction: 1xx Setup: 2xx Kubeflow: 3xx API Deployment: 4xx Help: 10xx  2. 기존의 포스트를 수정하는 경우 # 기존의 포스트를 수정할 경우 Contributor에 본인의 이름을 입력합니다.\ncontributors: [\u0026#34;John Doe\u0026#34;, \u0026#34;Adam Smith\u0026#34;] 3. 프로젝트에 처음 기여하는 경우 # 만약 프로젝트에 처음 기여 할 경우 content/en/contributors에 본인의 이름의 마크다운 파일을 작성합니다. 마크다운 파일은 john-doe을 파일명으로 하며 다음의 내용을 작성합니다. 파일 명은 lowercase를 title은 upper camelcase를 이용해 작성합니다.\n--- title: \u0026#34;Jonh Doe\u0026#34; draft: false --- Before Commit # 프로젝트에서는 각 글들의 일관성을 위해서 여러 lint를 적용하고 있습니다. 다음 명령어를 실행해 test를 진행합니다.\npre-commit을 통해 대부분의 test를 통과할 수 있습니다.\npip install pre-commit pre-commit run -a pre-commit 후 test를 진행합니다.\nnpm test "}).add({id:20,href:"/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener('input',b,!0);function b(){var b,e;const d=5;b=this.value,e=a.search(b,{limit:d,enrich:!0});const c=new Map;for(const a of e.flatMap(a=>a.result)){if(c.has(a.doc.href))continue;c.set(a.doc.href,a.doc)}if(suggestions.innerHTML="",suggestions.classList.remove('d-none'),c.size===0&&b){const a=document.createElement('div');a.innerHTML=`No results for "<strong>${b}</strong>"`,a.classList.add("suggestion__no-results"),suggestions.appendChild(a);return}for(const[h,g]of c){const b=document.createElement('div');suggestions.appendChild(b);const a=document.createElement('a');a.href=h,b.appendChild(a);const e=document.createElement('span');e.textContent=g.title,e.classList.add("suggestion__title"),a.appendChild(e);const f=document.createElement('span');if(f.textContent=g.description,f.classList.add("suggestion__description"),a.appendChild(f),suggestions.appendChild(b),suggestions.childElementCount==d)break}}})()